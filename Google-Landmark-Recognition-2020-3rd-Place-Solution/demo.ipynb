{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!ls -lrth ../input/b7ns-final-672-300w-f0-load13-load1-14ep/b7ns_final_672_300w_f0_load13_load1_14ep_fold0_ep4.pth\n",
    "!ls -lrth ../input/b6ns-final-768-300w-f1-load28-5ep-1e-5/b6ns_final_768_300w_f1_load28_5ep_1e-5_fold1_ep5.pth\n",
    "!ls -lrth ../input/b5ns-final-768-300w-f2-load16-20ep/b5ns_final_768_300w_f2_load16_20ep_fold2_ep1.pth\n",
    "!ls -lrth ../input/b4ns-final-768-300w-f0-load16-20ep-load1-20ep/b4ns_final_768_300w_f0_load16_20ep_load1_20ep_fold0_ep4.pth\n",
    "!ls -lrth ../input/b3ns-final-768-300w-f1-load29-5ep5ep/b3ns_final_768_300w_f1_load29_5ep5ep_fold1_ep5.pth\n",
    "!ls -lrth ../input/nest101-final-768-300w-f4-load16-19ep-load1-16ep/nest101_final_768_300w_f4_load16_19ep_load1_16ep_fold4_ep5.pth\n",
    "!ls -lrth ../input/rex20-ddp-final-768-300w-f4-35ep-load20resume/rex20_DDP_final_768_300w_f4_35ep_load20resume_fold4_ep31.pth\n",
    "!ls -lrth ../input/b6ns-ddp-final-512-300w-f1-40ep/b6ns_DDP_final_512_300w_f1_40ep_fold1_ep36.pth\n",
    "!ls -lrth ../input/b5ns-final-768-300w-f2-load33-5ep-3e-5-32g/b5ns_final_768_300w_f2_load33_5ep_3e-5_32G_fold2_ep4.pth"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "SKIP_COMMIT = False"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "import sys\n",
    "# sys.path = [\n",
    "#     '../input/geffnet-20200820',\n",
    "#     '../input/rexnetv1',\n",
    "#     '../input/resnest/ResNeSt-master'\n",
    "# ] + sys.path\n",
    "sys.path.append(\"/mnt/data/sjx/CS498_DL_Project\")\n",
    "# print(sys.path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "import math\n",
    "import pickle\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import albumentations\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from tqdm.notebook import tqdm as tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import geffnet"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "# data_dir = '../input/landmark-recognition-2020/'\n",
    "# model_dir = '../input/landmarkmodels/'\n",
    "data_dir = '/mnt/data/sjx/CS498_DL_Project/data/'\n",
    "model_dir = '/mnt/data/sjx/CS498_DL_Project/Google-Landmark-Recognition-2020-3rd-Place-Solution/weights/'\n",
    "df = pd.read_csv(os.path.join(data_dir, 'train.csv'))\n",
    "df['filepath'] = df['id'].apply(lambda x: os.path.join(data_dir, 'train', x[0], x[1], x[2], f'{x}.jpg'))\n",
    "df_demo_0 = df[df['filepath'].str.startswith('/mnt/data/sjx/CS498_DL_Project/data/train/0/0')]\n",
    "df_demo_1 = df[df['filepath'].str.startswith('/mnt/data/sjx/CS498_DL_Project/data/train/0/1')]\n",
    "df_demo_2 = df[df['filepath'].str.startswith('/mnt/data/sjx/CS498_DL_Project/data/train/0/2')]\n",
    "df_demo_3 = df[df['filepath'].str.startswith('/mnt/data/sjx/CS498_DL_Project/data/train/0/3')]\n",
    "\n",
    "df_demo = df_demo_0.append([df_demo_1, df_demo_2, df_demo_3])\n",
    "# get train and valid dataset\n",
    "df = df_demo\n",
    "df.to_csv('/mnt/data/sjx/CS498_DL_Project/data/tmp/train_demo.csv')\n",
    "\n",
    "\n",
    "df_sub = pd.read_csv(os.path.join(data_dir, 'sample_submission.csv'))\n",
    "\n",
    "df_test = df_sub[['id']].copy()\n",
    "df_test['filepath'] = df_test['id'].apply(lambda x: os.path.join(data_dir, 'test', x[0], x[1], x[2], f'{x}.jpg'))\n",
    "df_test.to_csv(os.path.join('/mnt/data/sjx/CS498_DL_Project', 'data/test.csv'), index = False)\n",
    "use_metric = True"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10345, 2)\n",
      "(10345, 2)\n"
     ]
    }
   ],
   "source": [
    "print(df_test.shape)\n",
    "print(df_sub.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "batch_size = 4\n",
    "num_workers = 4\n",
    "out_dim = 81313\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "transforms_672 = albumentations.Compose([\n",
    "    albumentations.Resize(672, 672),\n",
    "    albumentations.Normalize()\n",
    "])\n",
    "\n",
    "transforms_768 = albumentations.Compose([\n",
    "    albumentations.Resize(768, 768),\n",
    "    albumentations.Normalize()\n",
    "])\n",
    "transforms_512 = albumentations.Compose([\n",
    "    albumentations.Resize(512, 512),\n",
    "    albumentations.Normalize()\n",
    "])\n",
    "\n",
    "\n",
    "class LandmarkDataset(Dataset):\n",
    "    def __init__(self, csv, split, mode, transforms=[transforms_672, transforms_768,transforms_512]):\n",
    "\n",
    "        self.csv = csv.reset_index()\n",
    "        self.split = split\n",
    "        self.mode = mode\n",
    "        self.transform672 = transforms[0]\n",
    "        self.transform768 = transforms[1]\n",
    "        self.transform512 = transforms[2]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.csv.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.csv.iloc[index]\n",
    "        # print(row.filepath)\n",
    "        image = cv2.imread(row.filepath)\n",
    "        image = image[:, :, ::-1]\n",
    "\n",
    "        res0 = self.transform672(image=image)\n",
    "        image0 = res0['image'].astype(np.float32)\n",
    "        image0 = image0.transpose(2, 0, 1)\n",
    "\n",
    "        res1 = self.transform768(image=image)\n",
    "        image1 = res1['image'].astype(np.float32)\n",
    "        image1 = image1.transpose(2, 0, 1)\n",
    "\n",
    "        res3 = self.transform512(image=image)\n",
    "        image3 = res3['image'].astype(np.float32)\n",
    "        image3 = image3.transpose(2, 0, 1)\n",
    "\n",
    "\n",
    "        if self.mode == 'test':\n",
    "            return torch.tensor(image0), torch.tensor(image1) , torch.tensor(image3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "if df.shape[0] > 100001: # commit\n",
    "    df = df[df.index % 10 == 0].iloc[500:1000].reset_index(drop=True)\n",
    "    df_test = df_test.head(101).copy()\n",
    "\n",
    "dataset_query = LandmarkDataset(df, 'test', 'test')\n",
    "query_loader = torch.utils.data.DataLoader(dataset_query, batch_size=batch_size, num_workers=num_workers)\n",
    "\n",
    "dataset_test = LandmarkDataset(df_test, 'test', 'test')\n",
    "test_loader = torch.utils.data.DataLoader(dataset_test, batch_size=batch_size, num_workers=num_workers)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2587\n"
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([3, 672, 672]),\n torch.Size([3, 768, 768]),\n torch.Size([3, 512, 512]))"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# each group of data contains three scales of image: 672, 768, 512\n",
    "dataset_query[0][0].shape, dataset_query[0][1].shape, dataset_query[0][2].shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "class ArcMarginProduct_subcenter(nn.Module):\n",
    "    def __init__(self, in_features, out_features, k=3):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(out_features*k, in_features))\n",
    "        self.reset_parameters()\n",
    "        self.k = k\n",
    "        self.out_features = out_features\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, features):\n",
    "        cosine_all = F.linear(F.normalize(features), F.normalize(self.weight))\n",
    "        cosine_all = cosine_all.view(-1, self.out_features, self.k)\n",
    "        cosine, _ = torch.max(cosine_all, dim=2)\n",
    "        return cosine"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "sigmoid = torch.nn.Sigmoid()\n",
    "class Swish(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, i):\n",
    "        result = i * sigmoid(i)\n",
    "        ctx.save_for_backward(i)\n",
    "        return result\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        i = ctx.saved_variables[0]\n",
    "        sigmoid_i = sigmoid(i)\n",
    "        return grad_output * (sigmoid_i * (1 + i * (1 - sigmoid_i)))\n",
    "\n",
    "class Swish_module(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return Swish.apply(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class enet_arcface_FINAL(nn.Module):\n",
    "\n",
    "    def __init__(self, enet_type, out_dim):\n",
    "        super(enet_arcface_FINAL, self).__init__()\n",
    "        self.enet = geffnet.create_model(enet_type.replace('-', '_'), pretrained=None)\n",
    "        self.feat = nn.Linear(self.enet.classifier.in_features, 512)\n",
    "        self.swish = Swish_module()\n",
    "        self.metric_classify = ArcMarginProduct_subcenter(512, out_dim)\n",
    "        self.enet.classifier = nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.enet(x)\n",
    "        x = self.swish(self.feat(x))\n",
    "        return F.normalize(x), self.metric_classify(x)\n",
    "\n",
    "\n",
    "from rexnetv1 import ReXNetV1\n",
    "from resnest.torch import resnest101\n",
    "project_path = '/mnt/data/sjx/CS498_DL_Project'\n",
    "class rex20_arcface(nn.Module):\n",
    "\n",
    "    def __init__(self, enet_type, out_dim, load_pretrained=False):\n",
    "        super(rex20_arcface, self).__init__()\n",
    "        self.enet = ReXNetV1(width_mult=2.0)\n",
    "        if load_pretrained:\n",
    "            pretrain_wts = os.path.join(project_path, '/Google-Landmark-Recognition-2020-3rd-Place-Solution/weights/rexnetv1_2.0x.pth')\n",
    "            sd = torch.load(pretrain_wts)\n",
    "            self.enet.load_state_dict(sd, strict=True)\n",
    "\n",
    "        self.feat = nn.Linear(self.enet.output[1].in_channels, 512)\n",
    "        self.swish = Swish_module()\n",
    "        self.metric_classify = ArcMarginProduct_subcenter(512, out_dim)\n",
    "        self.enet.output = nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.enet(x)\n",
    "        if x.ndim==1:\n",
    "            x = x.unsqueeze(0)\n",
    "        x = self.swish(self.feat(x))\n",
    "        return F.normalize(x), self.metric_classify(x)\n",
    "\n",
    "class nest101_arcface(nn.Module):\n",
    "\n",
    "    def __init__(self, enet_type, out_dim):\n",
    "        super(nest101_arcface, self).__init__()\n",
    "        self.enet = resnest101(pretrained=False)\n",
    "        self.feat = nn.Linear(self.enet.fc.in_features, 512)\n",
    "        self.swish = Swish_module()\n",
    "        self.metric_classify = ArcMarginProduct_subcenter(512, out_dim)\n",
    "        self.enet.fc = nn.Identity()\n",
    "    def forward(self, x):\n",
    "        x = self.enet(x)\n",
    "        x = self.swish(self.feat(x))\n",
    "        return F.normalize(x), self.metric_classify(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "def load_model(model, model_file):\n",
    "    state_dict = torch.load(model_file)\n",
    "    if \"model_state_dict\" in state_dict.keys():\n",
    "        state_dict = state_dict[\"model_state_dict\"]\n",
    "    state_dict = {k[7:] if k.startswith('module.') else k: state_dict[k] for k in state_dict.keys()}\n",
    "#     del state_dict['metric_classify.weight']\n",
    "    model.load_state_dict(state_dict, strict=True)\n",
    "    # print(f\"loaded {model_file}\")\n",
    "    model.eval()\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!ls -lrth ../input/b7ns-final-672-300w-f0-load13-load1-14ep/b7ns_final_672_300w_f0_load13_load1_14ep_fold0_ep4.pth\n",
    "!ls -lrth ../input/b6ns-final-768-300w-f1-load28-5ep-1e-5/b6ns_final_768_300w_f1_load28_5ep_1e-5_fold1_ep5.pth\n",
    "!ls -lrth ../input/b5ns-final-768-300w-f2-load16-20ep/b5ns_final_768_300w_f2_load16_20ep_fold2_ep1.pth\n",
    "!ls -lrth ../input/b4ns-final-768-300w-f0-load16-20ep-load1-20ep/b4ns_final_768_300w_f0_load16_20ep_load1_20ep_fold0_ep4.pth\n",
    "!ls -lrth ../input/b3ns-final-768-300w-f1-load29-5ep5ep/b3ns_final_768_300w_f1_load29_5ep5ep_fold1_ep5.pth\n",
    "!ls -lrth ../input/nest101-final-768-300w-f4-load16-19ep-load1-16ep/nest101_final_768_300w_f4_load16_19ep_load1_16ep_fold4_ep5.pth\n",
    "!ls -lrth ../input/rex20-ddp-final-768-300w-f4-35ep-load20resume/rex20_DDP_final_768_300w_f4_35ep_load20resume_fold4_ep31.pth\n",
    "!ls -lrth ../input/b6ns-ddp-final-512-300w-f1-40ep/b6ns_DDP_final_512_300w_f1_40ep_fold1_ep36.pth\n",
    "!ls -lrth ../input/b5ns-final-768-300w-f2-load33-5ep-3e-5-32g/b5ns_final_768_300w_f2_load33_5ep_3e-5_32G_fold2_ep4.pth"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "# Here we just use one model instead of 9 to demo\n",
    "# model_b7 = enet_arcface_FINAL('tf_efficientnet_b7_ns', out_dim=out_dim).to(device)\n",
    "# model_b7 = load_model(model_b7, '../input/b7ns-final-672-300w-f0-load13-load1-14ep/b7ns_final_672_300w_f0_load13_load1_14ep_fold0_ep4.pth')\n",
    "#\n",
    "# model_b6 = enet_arcface_FINAL('tf_efficientnet_b6_ns', out_dim=out_dim).to(device)\n",
    "# model_b6 = load_model(model_b6, '../input/b6ns-final-768-300w-f1-load28-5ep-1e-5/b6ns_final_768_300w_f1_load28_5ep_1e-5_fold1_ep5.pth')\n",
    "#\n",
    "# model_b5 = enet_arcface_FINAL('tf_efficientnet_b5_ns', out_dim=out_dim).to(device)\n",
    "# model_b5 = load_model(model_b5, '../input/b5ns-final-768-300w-f2-load16-20ep/b5ns_final_768_300w_f2_load16_20ep_fold2_ep1.pth')\n",
    "#\n",
    "# model_b4 = enet_arcface_FINAL('tf_efficientnet_b4_ns', out_dim=out_dim).to(device)\n",
    "# model_b4 = load_model(model_b4, '../input/b4ns-final-768-300w-f0-load16-20ep-load1-20ep/b4ns_final_768_300w_f0_load16_20ep_load1_20ep_fold0_ep4.pth')\n",
    "#\n",
    "# model_b3 = enet_arcface_FINAL('tf_efficientnet_b3_ns', out_dim=out_dim).to(device)\n",
    "# model_b3 = load_model(model_b3, '../input/b3ns-final-768-300w-f1-load29-5ep5ep/b3ns_final_768_300w_f1_load29_5ep5ep_fold1_ep5.pth')\n",
    "\n",
    "# model_nest101 = nest101_arcface('nest101', out_dim=out_dim).to(device)\n",
    "# model_nest101 = load_model(model_nest101, '../input/nest101-final-768-300w-f4-load16-19ep-load1-16ep/nest101_final_768_300w_f4_load16_19ep_load1_16ep_fold4_ep5.pth')\n",
    "\n",
    "model_rex2 = rex20_arcface('rex2.0', out_dim=out_dim).to(device)\n",
    "model_rex2 = load_model(model_rex2, 'weights/rex20_DDP_final_768_300w_f4_50ep_fold4_final.pth')\n",
    "\n",
    "# model_b6b = enet_arcface_FINAL('tf_efficientnet_b6_ns', out_dim=out_dim).to(device)\n",
    "# model_b6b = load_model(model_b6b, '../input/b6ns-ddp-final-512-300w-f1-40ep/b6ns_DDP_final_512_300w_f1_40ep_fold1_ep36.pth')\n",
    "#\n",
    "# model_b5b = enet_arcface_FINAL('tf_efficientnet_b5_ns', out_dim=out_dim).to(device)\n",
    "# model_b5b = load_model(model_b5b, '../input/b5ns-final-768-300w-f2-load33-5ep-3e-5-32g/b5ns_final_768_300w_f2_load33_5ep_3e-5_32G_fold2_ep4.pth')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Nov  9 11:02:14 2020       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 418.56       Driver Version: 418.56       CUDA Version: 10.1     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  TITAN Xp            Off  | 00000000:05:00.0 Off |                  N/A |\r\n",
      "| 33%   52C    P2    64W / 250W |   4572MiB / 12194MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   1  TITAN Xp            Off  | 00000000:06:00.0 Off |                  N/A |\r\n",
      "| 25%   45C    P8    10W / 250W |     12MiB / 12196MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   2  TITAN Xp            Off  | 00000000:09:00.0 Off |                  N/A |\r\n",
      "| 28%   48C    P8    11W / 250W |     12MiB / 12196MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   3  TITAN Xp            Off  | 00000000:0A:00.0 Off |                  N/A |\r\n",
      "| 60%   85C    P2   111W / 250W |   5374MiB / 12196MiB |     97%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0      1842      G   /usr/lib/xorg/Xorg                            33MiB |\r\n",
      "|    0      6930      C   /home/jingxiang/anaconda3/bin/python        4527MiB |\r\n",
      "|    3      8662      C   python                                       149MiB |\r\n",
      "|    3     28551      C   python                                      5211MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0            27\n",
      "1            60\n",
      "2           124\n",
      "3           134\n",
      "4           136\n",
      "          ...  \n",
      "16850    202955\n",
      "16851    202967\n",
      "16852    202975\n",
      "16853    202984\n",
      "16854    203026\n",
      "Length: 16855, dtype: int64\n",
      "0            9\n",
      "1           23\n",
      "2           47\n",
      "3           49\n",
      "4           51\n",
      "         ...  \n",
      "16850    81265\n",
      "16851    81268\n",
      "16852    81272\n",
      "16853    81276\n",
      "16854    81284\n",
      "Length: 16855, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# exchange the key and value in idx2landmark_id\n",
    "with open(os.path.join('..', 'data', 'idx2landmark_id.pkl'), 'rb') as fp:\n",
    "    idx2landmark_id = pickle.load(fp)\n",
    "\n",
    "    landmark_id2idx = {idx2landmark_id[idx]: idx for idx in idx2landmark_id.keys()}\n",
    "\n",
    "\n",
    "pred_mask = pd.Series(df.landmark_id.unique()).map(landmark_id2idx).values\n",
    "a = pd.Series(df.landmark_id.unique())\n",
    "b = a.map(landmark_id2idx)\n",
    "print(a)\n",
    "print(b)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [
    {
     "data": {
      "text/plain": "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=2587.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "118f8db2eb944d3989a2da7797802bcc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(10345, 5)\n",
      "(10348, 5)\n"
     ]
    }
   ],
   "source": [
    "TOP_K = 5\n",
    "CLS_TOP_K = 5\n",
    "if True:\n",
    "    with torch.no_grad():\n",
    "        # feats = []\n",
    "        # for img0, img1,img3 in tqdm(query_loader): # 672, 768, 512\n",
    "        #     img0 = img0.cuda()\n",
    "        #     img1 = img1.cuda()\n",
    "        #     img3 = img3.cuda()\n",
    "        #\n",
    "        #     # feat_b7,_      = model_b7(img0)\n",
    "        #     # feat_b6,_      = model_b6(img1)\n",
    "        #     # feat_b5,_      = model_b5(img1)\n",
    "        #     # feat_b4,_      = model_b4(img1)\n",
    "        #     # feat_b3,_      = model_b3(img1)\n",
    "        #     # feat_nest101,_ = model_nest101(img1)\n",
    "        #     feat_rex2,_    = model_rex2(img1)\n",
    "        #     # feat_b6b,_     = model_b6b(img3)\n",
    "        #     # feat_b5b,_     = model_b5b(img1)\n",
    "        #     # feat = torch.cat([feat_b7,feat_b6,feat_b5,feat_b4,feat_b3,feat_nest101,feat_rex2,feat_b6b,feat_b5b],dim=1)\n",
    "        #     feat = feat_rex2\n",
    "        #     # print(feat.shape)\n",
    "        #     feats.append(feat.detach().cpu())\n",
    "        # feats = torch.cat(feats)\n",
    "        # feats = feats.cuda()\n",
    "        # feat = F.normalize(feat)\n",
    "\n",
    "        PRODS = []\n",
    "        PREDS = []\n",
    "        PRODS_M = []\n",
    "        PREDS_M = []\n",
    "        for img0, img1,img3 in tqdm(test_loader):\n",
    "            img0 = img0.cuda()\n",
    "            img1 = img1.cuda()\n",
    "            img3 = img3.cuda()\n",
    "\n",
    "            probs_m = torch.zeros([4, 81313],device=device)\n",
    "            # feat_b7,logits_m      = model_b7(img0); probs_m += logits_m\n",
    "            # feat_b6,logits_m      = model_b6(img1); probs_m += logits_m\n",
    "            # feat_b5,logits_m      = model_b5(img1); probs_m += logits_m\n",
    "            # feat_b4,logits_m      = model_b4(img1); probs_m += logits_m\n",
    "            # feat_b3,logits_m      = model_b3(img1) ; probs_m += logits_m\n",
    "            # feat_nest101,logits_m = model_nest101(img1); probs_m += logits_m\n",
    "            feat_rex2,logits_m    = model_rex2(img1); probs_m += logits_m\n",
    "            # feat_b6b,logits_m     = model_b6b(img3); probs_m += logits_m\n",
    "            # feat_b5b,logits_m     = model_b5b(img1) ; probs_m += logits_m\n",
    "            # feat = torch.cat([feat_b7,feat_b6,feat_b5,feat_b4,feat_b3,feat_nest101,feat_rex2,feat_b6b,feat_b5b],dim=1)\n",
    "\n",
    "            feat = F.normalize(feat_rex2)\n",
    "\n",
    "            # probs_m = probs_m/9\n",
    "            probs_m[:, pred_mask] += 1.0\n",
    "            probs_m -= 1.0\n",
    "\n",
    "            (values, indices) = torch.topk(probs_m, CLS_TOP_K, dim=1)\n",
    "            probs_m = values\n",
    "            preds_m = indices\n",
    "\n",
    "            PRODS_M.append(probs_m.detach().cpu())\n",
    "            PREDS_M.append(preds_m.detach().cpu())\n",
    "\n",
    "            distance = feat.mm(feats.t())\n",
    "            (values, indices) = torch.topk(distance, TOP_K, dim=1)\n",
    "            probs = values\n",
    "            preds = indices\n",
    "\n",
    "            PRODS.append(probs.detach().cpu())\n",
    "            PREDS.append(preds.detach().cpu())\n",
    "\n",
    "\n",
    "        PRODS = torch.cat(PRODS).numpy()\n",
    "        PREDS = torch.cat(PREDS).numpy()\n",
    "        PRODS_M = torch.cat(PRODS_M).numpy()\n",
    "        PREDS_M = torch.cat(PREDS_M).numpy()\n",
    "\n",
    "print(PREDS.shape)\n",
    "print(PREDS_M.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jingxiang/anaconda3/lib/python3.8/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "f = open(os.path.join(project_path, 'data', 'PREDS.pkl'), 'wb')\n",
    "pickle.dump(PREDS, f)\n",
    "f = open(os.path.join(project_path, 'data', 'PREDS_M.pkl'), 'wb')\n",
    "pickle.dump(PREDS_M, f)\n",
    "f = open(os.path.join(project_path, 'data', 'feats.pkl'), 'wb')\n",
    "pickle.dump(feats, f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [],
   "source": [
    "# map both to landmark_id\n",
    "PREDS_M = PREDS_M[:10345]\n",
    "gallery_landmark = df['landmark_id'].values\n",
    "PREDS = gallery_landmark[PREDS]\n",
    "PREDS_M = np.vectorize(idx2landmark_id.get)(PREDS_M)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10345, 5)\n",
      "(10345, 5)\n"
     ]
    }
   ],
   "source": [
    "print(PREDS.shape)\n",
    "print(PREDS_M.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [
    {
     "data": {
      "text/plain": "(9, 203026, 14632, 201896)"
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PREDS.min(), PREDS.max(), PREDS_M.min(), PREDS_M.max()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[203007, 151352,  77052,  32057, 125696],\n       [ 98993,  33770, 160244, 203026,  83663],\n       [203007, 151352,  77052,  32057, 125696]])"
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PREDS[:3,:]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "PREDS_M[:3,:]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "PRODS[:3,:]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[-0.9171716 , -0.9218958 , -0.9240964 , -0.9260289 , -0.9333472 ],\n       [-0.9166362 , -0.92121005, -0.92343813, -0.92565674, -0.9329817 ],\n       [-0.92858416, -0.9345174 , -0.93821883, -0.94038534, -0.9417717 ]],\n      dtype=float32)"
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PRODS_M[:3,:]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [
    {
     "data": {
      "text/plain": "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=10345.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0260f027de124807a0459da9c8bf17c8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "PRODS_F = []\n",
    "PREDS_F = []\n",
    "for i in tqdm(range(PREDS.shape[0])):\n",
    "    tmp = {}\n",
    "    classify_dict = {PREDS_M[i,j] : PRODS_M[i,j] for j in range(CLS_TOP_K)}\n",
    "    for k in range(TOP_K):\n",
    "        lid = PREDS[i, k]\n",
    "        tmp[lid] = tmp.get(lid, 0.) + float(PRODS[i, k]) ** 9 * classify_dict.get(lid,1e-8)**10\n",
    "    pred, conf = max(tmp.items(), key=lambda x: x[1])\n",
    "    PREDS_F.append(pred)\n",
    "    PRODS_F.append(conf)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5174\n"
     ]
    },
    {
     "data": {
      "text/plain": "[203007, 98993, 203007, 98993, 203007, 98993, 203007, 98993, 203007, 98993]"
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(PREDS_F))\n",
    "PREDS_F[:10]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "data": {
      "text/plain": "[1.0000010728841176e-80,\n 1.0000010728841176e-80,\n 1.0000010728841176e-80,\n 1.0000010728841176e-80,\n 1.0000010728841176e-80,\n 1.0000010728841176e-80,\n 1.0000010728841176e-80,\n 1.0000010728841176e-80,\n 1.0000010728841176e-80,\n 1.0000010728841176e-80]"
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PRODS_F[:10]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'filepath'], dtype='object')\n",
      "(10345, 2)\n"
     ]
    }
   ],
   "source": [
    "print(df_test.keys())\n",
    "print(df_test.shape)\n",
    "df_test['pred_id'] = PREDS_F\n",
    "df_test['pred_conf'] = PRODS_F"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [],
   "source": [
    "df_sub['landmarks'] = df_test.apply(lambda row: f'{row[\"pred_id\"]} {row[\"pred_conf\"]}', axis=1)\n",
    "df_sub.to_csv('submission.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [
    {
     "data": {
      "text/plain": "                 id                      landmarks\n0  00084cdf8f600d00  180612 1.0000000000000001e-80\n1  000b15b043eb8cf0   39391 1.0000000000000001e-80\n2  0011a52f9b948fd2      138982 0.4766400934727725\n3  00141b8a5a729084  171801 1.0000000000000001e-80\n4  0018aa4b92532b77   123495 9.992813975943116e-81",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>landmarks</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>00084cdf8f600d00</td>\n      <td>180612 1.0000000000000001e-80</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000b15b043eb8cf0</td>\n      <td>39391 1.0000000000000001e-80</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0011a52f9b948fd2</td>\n      <td>138982 0.4766400934727725</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>00141b8a5a729084</td>\n      <td>171801 1.0000000000000001e-80</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0018aa4b92532b77</td>\n      <td>123495 9.992813975943116e-81</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sub.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}